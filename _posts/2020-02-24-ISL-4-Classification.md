---
layout: post
title: ISL 4. Classification
subtitle: An Introduction to Statistical Learning
tags: [ISL]
use_math: true
---





이번 장에서는 질적(범주적) 반응변인을 예측하기 위하여 사용되는 **분류(classification)**에 대하여 공부한다. 즉, 어떤 관측치를 하나의 범주나 클래스에 할당하는 방법으로, 이번 챕터에서는 먼저 로지스틱 회귀(logistic regression), 선형 판별 분석(linear discriminant analysis)에 대하여 학습한다. 이후 챕터들에서 일반화 가법 모형(generalized additive model), 트리 기반 모델, 랜덤 포레스트, 부스팅, SVM 등의 더 많은 분류 기법에 대하여 다루게 될 것이다. K-최근접 이웃(K-nearest neighbors)에 대한 내용은 챕터 2에서 다룬 바 있다.

지난 챕터에서 학습했던 선형 회귀를 왜 분류 문제에는 적용할 수 없는지 먼저 알아보고, 분류에 활용되는 **로지스틱 회귀**를 소개할 것이다. 로지스틱 회귀에서는 예측변인과 반응변인의 관계가 선형적이지 않으며, 최대 우도 추정법을 통해 모수를 추정할 수 있다. 예측변인이 주어졌을 때 반응변인에 대한 조건부 분포를 모델링하는 로지스틱 회귀와 달리, **선형 판별 분석(LDA)**은 거꾸로 반응변인이 주어졌을 때 예측변인의 분포를 모델링하고 베이즈 정리를 이용하여 사후 확률을 계산한다. LDA는 각 클래스의 관측치들이 클래스 고유의 평균 벡터와 모든 클래스들의 공통 분산을 가지는 정상분포로부터 나왔다고 가정하고 모수를 추정한다. 한편, **이차 판별 분석(quadratic discriminant analysis)**은 LDA와 달리 클래스마다 다른 공분산을 가진다고 가정하기 때문에 선형 결정 경계를 갖는 LDA에 비해 분산이 높고 편향이 낮다.

이번 챕터에서는 분류에서 자주 쓰이는 개념들에 대해서도 짚고 넘어간다. 이진 분류에서는 두 가지 종류의 오류, 즉 음성을 양성으로 잘못 분류하는 위양성(false positive)과 양성을 음성으로 잘못 분류하는 위음성(false negative)이 발생할 수 있다. 이때, 실제 양성을 양성으로 올바르게 분류한 비율을 **민감도(sensitivity)** 혹은 **재현율(recall)**이라 하며, 양성으로 분류된 것 중 실제 양성의 비율은 **정밀도(precision)**, 그리고 실제 음성을 음성으로 올바르게 분류한 비율은 **특이도(specificity)**라 한다. 오차 행렬(confusion matrix)과 ROC 커브를 통해 이를 확인해볼 수 있다.



## 1 왜 선형 회귀를 사용할 수 없는가?

질적 반응변인을 예측하는 데에는 선형 회귀를 사용할 수 없다. 우선 질적 반응변인이 세 가지 이상의 수준을 가지는 경우에는 그것을 하나의 양적인 반응으로 변환하여 사용할 수 없다. 예를 들어, 세 가지 수준에 0, 1, 2라는 숫자를 부여한다고 할 때 그 순서가 결과에 영향을 미치기 때문이다. 수준이 두 가지인 이진(binary) 반응을 더미변수로 만드는 경우에도 문제가 발생한다. 예측치가 [0,1]의 범위를 벗어날 수 있기 때문에 이것을 확률로 해석하기 어려워진다.



## 2 로지스틱 회귀



### 2.1 로지스틱 모델

그렇다면 $$X$$와 $$X$$일 때 $$Y$$의 클래스가 1일 확률 $$p(X) = Pr(Y = 1|X)$$ 간의 관계를 어떻게 모델로 만들 수 있을까? 아래와 같은 로지스틱 함수를 사용할 수 있다.</br>
$$
p(X) = \frac {e^{β_0+β_1X}} {1 + e^{β_0+β_1X}}
$$
로지스틱 함수는 항상 S자 모양의 곡선을 그리기 때문에 $$X$$의 값에 상관없이 합당한 범위에 있는 예측을 할 수 있다. 위 모델을 조금 변형하면 아래와 같은 식을 얻을 수 있다.</br>
$$
\frac {p(X)} {1 − p(X)} = e^{β_0+β_1X}
$$
이것을 **오즈(odds)**라고 하며 0과 $$∞$$ 사이의 값을 가진다. 오즈 공식의 양변에 로그를 씌워주면 아래와 같은 식을 얻는다.</br>
$$
\log \left(\frac {p(X)} {1 − p(X)} \right) = β_0+β_1X
$$
이것을 **로그-오즈(log-odds)** 혹은 **로짓(logit)**이라고 하며 $$-∞$$과 $$∞$$ 사이의 값을 가진다. 위 식에서 로짓이 $$X$$에 대하여 선형이라는 것을 알 수 있다. 이때 $$X$$에서의 한 단위 변화는 $$p(X)$$가 아니라 로짓을 $$β_1$$만큼, 오즈를 $$e^{β_1}$$을 곱한 것만큼 변화시킨다. 따라서 $$X$$와 $$p(X)$$의 관계는 선형이 아니며 $$X$$에서의 변화가 $$p(X)$$에 미치는 영향은 $$X$$의 현재 값에 따라 좌우된다.



### 2.2 회귀 계수 추정하기

계수 $$β_0$$와 $$β_1$$은 미지의 모수이기 때문에 학습 데이터를 통해 추정되어야 한다. 이때 최소자승법을 사용할 수 없고 **최대 우도(maximum likelihood)** 추정법을 사용하여 예측된 확률 $$\hat{p}(x_i)$$가 실제 관찰된 클래스에 잘 부합하도록 $$β_0$$와 $$β_1$$을 추정해야 한다. 우도 함수(likelihood function)는 수학적으로 아래와 같이 정의된다.</br>
$$
\mathcal{L} (β_0, β_1) = \prod_{i:y_i=1} p(x_i) \prod_{i':y_{i'}=0} (1 − p(x_{i'}))
$$
이 우도 함수를 최대화할 수 있는 추정값 $$\hat{β}_0$$와 $$\hat{β}_1$$이 선택된다. 최대 우도 추정법은 로지스틱 회귀 외에도 비선형 모델을 적합시키는 데 흔히 사용된다.



### 2.3 다중 로지스틱 회귀

로짓의 개념을 복수의 예측변인에 대하여 확장하면 다음과 같다.</br>
$$
\log \left(\frac {p(X)} {1 − p(X)} \right) = β_0+β_1X_1+...+β_pX_p
$$
이때 $$X = (X_1, . . .,X_p)$$는 $$p$$개의 예측변인을 말한다. 위 식은 다시 아래와 같이 정리될 수 있다.</br>
$$
p(X) = \frac {e^{β_0+β_1X_1+...+β_pX_p}} {1 + e^{β_0+β_1X_1+...+β_pX_p}}
$$
동일하게 최대 우도 추정법을 사용해 $$β_0, β_1, . . . , β_p$$를 추정하면 된다.

다중 로지스틱 회귀에서 구해진 계수의 추정치는 단순 로지스틱 회귀에서 구해진 추정치와 다를 수 있다. 즉, 다중 로지스틱 회귀로 다른 예측변인들이 모두 통제된 상황에서 특정 예측변인이 반응변인에 미치는 영향을 추정한 값과 단순 로지스틱 회귀로 해당 변인이 혼자 반응변인에 미치는 영향을 추정한 값이 다를 수 있다. 특히 예측변인 간 상관이 존재하는 경우 두 추정치가 차이를 보인다.



### 2.4 반응변인의 클래스가 2개보다 많은 경우

위에서 다룬 이진 클래스의 로지스틱 회귀 모델은 다중 클래스를 예측하는 것으로 확장될 수 있다. 그러나 다중 클래스는 로지스틱 회귀보다는 선형 판별 분석으로 다루는 것이 더 흔하다.



## 3 선형 판별 분석

로지스틱 회귀가 예측변인 $$X$$가 주어졌을 때 반응변인 $$Y$$에 대한 조건부 분포를 모델링하는 반면, **선형 판별 분석(Linear Discriminant Analysis; LDA)**은 $$Y$$가 주어졌을 때 각 범주에서의 예측변인 $$X$$의 분포를 모델링하고, 베이즈 정리를 통해 이것을 $$Pr(Y = k|X = x)$$에 대한 추정치로 전환한다.

로지스틱 회귀에 비해 LDA의 장점은 다음과 같다. 첫째, 클래스가 잘 분리되어 있을 때 로지스틱 회귀 모델에서는 파라미터 추정치들이 매우 불안정한데 LDA는 이런 문제를 겪지 않는다. 둘째, $$n$$이 작고 예측변인 $$X$$의 분포가 각 클래스에서 정상분포에 근사하는 경우 로지스틱 회귀 모델에 비해 LDA가 더 안정적이다. 셋째, 반응 클래스가 2개보다 많다면 LDA가 더 흔하게 사용된다.



### 3.1 분류 문제에 베이즈 정리 사용하기

어떤 관측치를 $$K$$개의 클래스 중 하나로 분류하고 싶다고 가정하자. (단, $$K ≥ 2$$) 즉, 질적인 반응변인 $$Y$$는 $$K$$개의 순서 없이 구분되는 값을 취할 수 있다. 이때, 무선적으로 선택된 관측치가 $$k$$번째 클래스에서 나왔을 **사전(prior)** 확률을 $$π_k$$라고 하자. $$f_k(X) ≡ Pr(X = x|Y = k)$$는 $$k$$번째 클래스에 속하는 어떤 관측치의 $$X$$의 밀도함수를 나타낸다. 만일 $$k$$번째 클래스에서 어떤 관측치가 $$X ≈ x$$를 가질 확률이 높다면 $$f_k(x)$$ 역시 클 것이며, $$k$$번째 클래스에서 어떤 관측치가 $$X ≈ x$$를 가질 확률이 낮다면 $$f_k(x)$$도 작을 것이다. **베이즈 정리**에 따르면 아래와 같은 식이 성립된다.</br>
$$
Pr(Y = k|X = x) = \frac {π_kf_k(x)} {\sum^K_{l=1} π_lf_l(x)}
$$
간편하게 $$p_k(X) = Pr(Y = k|X)$$라 할 때, $$p_k(x)$$를 어떤 관측치의 예측변인 값이 주어졌을 때 해당 관측치가 $$k$$번째 클래스에 속하는 **사후(posteior)** 확률이라고 한다. LDA에서는 로지스틱 모델에서와 같이 $$p_k(x)$$를 직접 계산하는 대신, 위의 베이즈 정리에 $$π_k$$와 $$f_k(X)$$의 추정치를 넣어서 계산한다. 모집단으로부터 $$Y$$의 무선적인 표본을 가지고 있기 때문에 $$π_k$$를 추정하기 위해서는 간단히 $$k$$번째 클래스에 속하는 훈련 관측치의 개수가 전체 중 몇 개인지 계산해주면 된다. 반면, $$f_k(X)$$를 추정하는 것은 쉬운 일이 아니다.

챕터 2에서 가장 이상적인 분류기로 상정되는 베이즈 분류기는 $$p_k(X)$$가  최대화되는 클래스에 관측치를 할당한다고 설명한 바 있다. $$f_k(X)$$를 추정할 수 있는 좋은 방법을 찾는다면 이러한 베이즈 분류기에 근사하는 분류 방법을 개발할 수 있을 것이다.



### 3.2 $$p = 1$$일 때의 선형 판별 분석

$$f_k(X)$$를 추정하기에 앞서 그것의 형태에 대한 가정이 필요하다. $$f_k(X)$$가 정상분포를 따른다고 가정해보자. 일차원에서 정상분포의 밀도함수는 아래와 같다.</br>
$$
f_k(x) = \frac {1} {\sqrt{2π}σ_k} \exp \left(− \frac {1} {2σ^2_k} (x − μ_k)^2 \right)
$$
이때, $$μ_k$$와 $$σ^2_k$$은 $$k$$번째 클래스에 대한 평균과 분산이다. 또한, $$σ^2_1 = . . . = σ^2_K$$, 즉 모든 $$K$$개의 클래스가 같은 분산 항 $$σ^2$$을 공유한다는 가정이 추가로 필요하다. 위 확률밀도를 앞 절에서 살펴본 베이즈 정리에 넣으면 아래와 같은 식을 도출할 수 있다.</br>
$$
p_k(x) = \frac {π_k \frac {1} {\sqrt{2π}σ} \exp \left(− \frac {1} {2σ^2} (x − μ_k)^2 \right)} {\sum^K_{l=1} π_l \frac {1} {\sqrt{2π}σ} \exp \left(− \frac {1} {2σ^2} (x − μ_l)^2 \right)}
$$
베이즈 분류기는 위 식이 최대화되는 클래스에 관측치 $$X = x$$를 할당할 것이다. 이것은 위 식에 로그를 취하고 항들을 정리해보면,</br>
$$
δ_k(x) = x · \frac{μ_k}{σ^2} − \frac {μ^2_k} {2σ^2} + \log(π_k)
$$
를 최대화하는 클래스에 관측치를 할당하는 것과 같다. 예를 들어, $$K = 2$$이고 $$π_1 = π_2$$라면, 두 클래스에 대하여,</br>
$$
δ_1(x) = x · \frac{μ_1}{σ^2} − \frac {μ^2_1} {2σ^2} + \log(π_1) \\
δ_2(x) = x · \frac{μ_2}{σ^2} − \frac {μ^2_2} {2σ^2} + \log(π_2)
$$
위 두 식을 비교하여 베이즈 분류기는 $$δ_1(x)>δ_2(x)$$, 정리해보면 $$2x (μ_1 − μ_2) > μ^2_1 − μ^2_2$$일 때 관측치를 클래스 1에, 그 외의 경우에는 클래스 2에 할당할 것이다. 이때, 베이즈 결정 경계는 아래에 해당한다.</br>
$$
x = \frac {μ^2_1 − μ^2_2} {2(μ_1 − μ_2)} = \frac{μ_1 + μ_2} {2}
$$
그러나 실제로 베이즈 분류기를 계산하는 것은 불가능하기 때문에 LDA에서는 $$π_k$$, $$μ_k$$, $$σ^2$$의 추정치를 위 $$δ_k(x)$$ 식에 넣어 베이즈 분류기를 모사하고자 한다. 구체적으로 아래의 추정치들이 사용된다.</br>
$$
\hat{μ}_k = \frac {1} {n_k} \sum_{i:y_i=k} x_i \\
\hat{σ}^2 = \frac {1} {n-K} \sum^K_{k=1} \sum_{i:y_i=k} (x_i − \hat{μ}_k)^2
$$
여기에서 $$n$$은 전체 훈련 관측치의 개수이며, $$n_k$$는 $$k$$번째 클래스에 속하는 훈련 관측치의 개수이다. 즉, $$μ_k$$의 추정치는 $$k$$번째 클래스에서의 모든 훈련 관측치들의 평균이고, $$σ^2$$의 추정치는 각 $$K$$개의 클래스에 대한 표본 분산의 가중 평균으로 볼 수 있다. 각 클래스에 속할 사전 확률 $$π_1, . . . , π_K$$에 대한 지식을 가지고 있다면 이를 바로 사용하면 되지만, 그렇지 않은 경우 LDA에서는 $$k$$번째 클래스에 속한 훈련 관측치의 비율로 $$π_k$$를 추정한다. 즉,</br>
$$
\hat{π}_k = n_k/n
$$
이다. 이러한 추정치들을 사용해 LDA는</br>
$$
\hat{δ}_k(x) = x · \frac{\hat{μ}_k}{\hat{σ}^2} − \frac {\hat{μ}^2_k} {2\hat{σ}^2} + \log(\hat{π}_k)
$$
가 최대가 되는 클래스에 관측치 $$X = x$$를 할당한다. 판별(discriminant) 함수 $$\hat{δ}_k(x)$$가 $$x$$에 대한 선형 함수라는 데에서 '선형' 판별 분석이라는 이름이 붙여졌다.

정리하자면, LDA 분류기는 각 클래스에 속하는 관측치들이 클래스 고유의 평균 벡터와 공통 분산 $$σ^2$$을 가지는 정상분포로부터 나왔다고 가정하고, 이 모수들에 대한 추정치를 이용해 베이즈 분류기를 근사한다.



### 3.3 $$p > 1$$일 때의 선형 판별 분석

$$X = (X_1,X_2, . . .,X_p)$$가 클래스 고유의 평균 벡터와 공통 공분산 행렬을 가지는 다변량(multivariate) 정상분포로부터 나왔다고 가정하자. 다변량 정상분포는 개별 예측변인들이 일차원 정상분포를 따르며, 예측변인 간의 상관을 가정한다. $$p$$차원의 확률변수 $$X$$가 다변량 정상분포를 가진다는 것을 표현하기 위하여 $$X ∼ N(μ,Σ)$$로 쓴다. 이때, $$E(X) = μ$$는 $$p$$개의 성분을 가지는 벡터 $$X$$의 평균이고, $$Cov(X) = Σ$$는 $$X$$의 $$p × p$$ 공분산 행렬이다. 다변량 정상분포의 밀도함수는 아래와 같이 정의된다.</br>
$$
f(x) = \frac {1} {(2π)^{p/2}|Σ|^{1/2}} \exp \left( -\frac {1}{2} (x − μ)^T Σ^{−1}(x − μ) \right)
$$
$$p > 1$$개의 예측변인이 존재하는 경우, LDA 분류기는 $$k$$번째 클래스의 관측치들이 다변량 정상분포 $$N(μ_k,Σ)$$에서 나왔다고 가정한다. 이때, $$μ_k$$는 클래스 고유의 평균 벡터이며, $$Σ$$는 모든 $$K$$개의 클래스가 공유하는 공분산 행렬이다. $$k$$번째 클래스에 대한 밀도함수 $$f_k(X = x)$$를 베이즈 정리에 넣고 대수(algebra)를 사용해 항들을 정리하면 베이즈 분류기는,</br>
$$
δ_k(x) = x^T Σ^{-1}μ_k − \frac {1}{2} μ^T_k Σ^{-1} μ_k + \logπ_k
$$
를 최대화하는 클래스에 관측치 $$X = x$$를 할당한다는 것을 알 수 있다. 윗 절에서 소개했던 식의 벡터/행렬 버전이라고 할 수 있다. 마찬가지로, LDA는 미지의 모수인 $$μ_1, . . . , μ_K$$, $$π_1, . . . , π_K$$, $$Σ$$를 추정하여 위 식에 넣고 $$\hat{δ}_k(x)$$를 최대화하는 클래스로 관측치를 분류한다.

다음 절로 넘어가기 전에, 분류에서 자주 쓰이는 개념들을 짚고 넘어갈 필요가 있다. 개인이 카드 대금을 연체하는지 예측하기 위하여 LDA를 수행한 결과, 2.75%의 훈련 오류율이 나왔다고 하자. 과연 이 오류율을 낮다고 할 수 있을까? 전체 데이터 중 연체자의 비율이 3.33%밖에 되지 않는다면 어떨까? 만일 어떤 분류기가 모든 개인에 대하여 무조건 연체를 하지 않는다고 예측하더라도 오류율은 3.33%밖에 되지 않을 것이다. 이진(binary) 분류기는 두 가지 종류의 오류를 범할 수 있다. **오차 행렬(confusion matrix)**은 이중 어떤 오류가 발생했는지 확인할 수 있는 좋은 방법이다. 연체 데이터에 대한 LDA의 오차 행렬은 다음과 같다.

|                    | 실제로 연체 아님 | 실제로 연체 | 합계   |
| ------------------ | ---------------- | ----------- | ------ |
| 연체 아님으로 예측 | 9,644            | 252         | 9,896  |
| 연체로 예측        | 23               | 81          | 104    |
| 합계               | 9,677            | 333         | 10,000 |

오차 행렬을 확인해 보면, 전반적인 오류율은 낮더라도 연체자에 대한 오류율($$252/333 = 75.7 \%$$)은 매우 높다는 것을 알 수 있다. 이때, 올바르게 예측된 연체자의 비율($$24.3\%$$)을 **민감도(sensitivity)**라고 하며, 올바르게 예측된 연체를 하지 않은 개인의 비율($$(1 − 23/9, 667)× 100 = 99.8 \%$$)을 **특이도(specificity)**라고 한다.

그렇다면 왜 이렇게 낮은 민감도가 나타난 것일까? LDA는 베이즈 분류기를 근사하는데, 이것은 클래스에 상관없이 오분류된 관측치의 '전체' 개수를 최대한 줄이고자 하기 때문이다. 베이즈 분류기와 LDA는 관측치를 어떤 클래스로 분류하기 위해서 50%의 **임계치(threshold)**를 사후 확률에 적용한다. 민감도를 높이고 싶다면 이러한 임계치를 낮추는 방법을 사용할 수 있다. 20%의 임계치를 사용했을 때 연체 데이터에 대한 LDA의 오차 행렬은 다음과 같다.

|                    | 실제로 연체 아님 | 실제로 연체 | 합계   |
| ------------------ | ---------------- | ----------- | ------ |
| 연체 아님으로 예측 | 9,432            | 138         | 9,570  |
| 연체로 예측        | 235              | 195         | 430    |
| 합계               | 9,677            | 333         | 10,000 |

이와 같이, 사후 확률의 임계치를 조정함에 따라 오류율에서는 트레이드오프가 발생한다. 임계치가 0.5일 때 전체 오류율은 가장 낮았지만 연체자에 대한 오류율은 다소 높았다. 임계치가 낮아짐에 따라 연체자에 대한 오류율은 감소하는 반면, 연체하지 않은 개인들에 대한 오류율은 증가할 것이다. 어떤 임계치가 가장 이상적인지는 도메인 지식에 따라 결정된다.

지금까지 배운 내용을 아래와 같이 정리해 보자.

|                         | True class = $$-$$  | True class = $$+$$  | Total |
| ----------------------- | ------------------- | ------------------- | ----- |
| Predicted class = $$-$$ | True Negative (TN)  | False Negative (FN) | N*    |
| Predicted class = $$+$$ | False Positive (FP) | True Positive (TP)  | P*    |
| Total                   | N                   | P                   |       |

위양성율(false positive rate)은 $$FP/N$$이며, 제1종 오류(Type I error)라 불리고 $$1-$$**특이도(specificity)**와 같다. 특이도는 진음성율(true negative rate)인 $$TN/N$$이다. 진양성율(true positive rate)은 $$TP/P$$이며, **민감도(sensitivity)** 혹은 **재현율(recall)**이라고 불린다. 한편, Positive Predictive value는 $$TP/P*$$이며, **정밀도(precision)**라고 불린다. Negative Predictive value는 $$TN/N*$$이다.

한편, **ROC 커브**는 가로축에 위양성율, 세로축에 진양성율을 가지며, 어떤 분류기의 전반적인 성능은 ROC 아래의 면적을 일컫는 **AUC(area under the curve)**로 확인할 수 있다. AUC가 클수록 성능이 좋으며 면적이 0.5이면 우연 수준의 성능을 보인다고 할 수 있다.



### 3.4 이차 판별 분석

LDA와 마찬가지로 **이차 판별 분석(quadratic discriminant analysis; QDA)** 역시 각 클래스의 관측치들이 정상분포로부터 나왔다고 가정하며, 예측을 하기 위해 모수에 대한 추정치들을 베이즈 정리에 넣는다. 그러나 QDA는 LDA와 달리 각 클래스가 고유의 공분산 행렬을 가진다고 가정한다. 즉, $$k$$번째 클래스의 관측치가 $$X ∼ N(μ_k,Σ_k)$$의 형태를 취한다고 가정한다. 이때, $$Σ_k$$는 $$k$$번째 클래스에 대한 공분산 행렬이다. 이러한 가정 아래 베이즈 분류기는 관측치 $$X = x$$를,</br>
$$
\begin{aligned}
δ_k(x) 
&= -\frac{1}{2} (x − μ_k)^TΣ_k^{−1}(x − μ_k) -\frac{1}{2} \log |Σ_k| + \log π_k \\
&= -\frac{1}{2} x^T Σ_k^{−1}x + x^T Σ_k^{−1} μ_k -\frac{1}{2} μ^T_k Σ_k^{−1} μ_k -\frac{1}{2} \log |Σ_k| + \log π_k
\end{aligned}
$$
가 최대화되는 클래스로 분류한다. 따라서 QDA 분류기는 위 식에 $$Σ_k$$, $$μ_k$$, $$π_k$$의 추정치를 넣고 그 값이 가장 큰 클래스로 관측치 $$X = x$$를 할당한다. LDA에서와 달리 위 식은 $$x$$에 대한 이차함수이기 때문에 '이차' 판별 분석이라는 이름이 붙었다.

$$p$$개의 예측변인이 존재하는 경우, 공분산 행렬을 추정하기 위해서는 $$p(p+1)/2$$개의 파라미터를 추정해야 한다. QDA는 각 클래스에 대하여 개별적으로 공분산 행렬을 추정하기 때문에 파라미터의 개수는 $$Kp(p+1)/2$$개가 될 것이다. 반면, LDA 모델은 $$x$$에 대하여 선형이기 때문에 $$Kp$$개의 선형 계수만 추정하면 된다. 따라서 QDA에 비해 LDA가 덜 유연하고 낮은 분산을 가진다. 만일 데이터의 크기가 작아 분산을 줄이는 것이 중요하다면 LDA를 사용하는 것이 좋을 것이고, 데이터가 충분히 크다면 개별 클래스마다 공분산을 다르게 가정하는 QDA가 더 바람직할 것이다.



## 4 분류 방법들의 비교

로지스틱 회귀와 LDA는 밀접한 연관을 가진다. $$p=1$$인 예측변인과 두 개의 클래스가 존재할 때,  $$p_1(x)$$과 $$p_2(x) = 1−p_1(x)$$를 각각 클래스 1와 클래스 2에 관측치 $$X = x$$가 속할 확률이라고 해보자. LDA에서 로그 오즈는 아래와 같다.</br>
$$
\log \left(\frac {p_1(x)} {1 − p_1(x)} \right)
= \log \left(\frac {p_1(x)} {p_2(x)} \right)
= c_0 + c_1x
$$
이때, $$c_0$$와 $$c_1$$은 $$μ_1$$, $$μ_2$$, $$σ^2$$에 대한 함수들이다. 로지스틱 회귀에서 로그 오즈는 아래와 같다.</br>
$$
\log \left(\frac {p_1} {1 − p_1} \right)
= \beta_0 + \beta_1x
$$
따라서 로지스틱 회귀와 LDA 모두 선형 결정 경계를 갖는다. 두 방법의 유일한 차이는 $$β_0$$와 $$β_1$$은 최대 우도를 사용해 추정되는 반면, $$c_0$$와 $$c_1$$은 정상분포의 평균과 분산을 추정해 계산된다는 것이다. $$p > 1$$일 때도 마찬가지이다. LDA가 필요로 하는 전제조건이 맞는다면 LDA는 로지스틱 회귀보다 좋은 수행을 보인다. 그렇지 않다면 로지스틱 회귀의 수행이 더 좋을 것이다.

챕터 2에서 살펴본 바 있는 KNN은 완전히 다른 접근법을 취한다. 결정 경계의 형태에 대한 어떠한 가정도 하지 않는 비모수적 방법이다. 결정 경계가 매우 비선형적인 경우 KNN은 LDA나 로지스틱 회귀에 비해 매우 우수한 수행을 보인다. QDA는 비모수적인 KNN 방법과 선형적인 LDA와 로지스틱 회귀 간의 절충안이라고 할 수 있다. 모든 상황에서 항상 좋은 성능을 내는 하나의 방법은 존재하지 않기 때문에 데이터의 속성에 따라 적절한 분류 방법을 선택해야 할 것이다.

마지막으로, 회귀에서와 같이, 예측변인과 반응변인 간의 비선형적 관계에 대응하기 위하여 예측변인을 변환할 수 있다. 예를 들어, 로지스틱 회귀에 $$X^2$$, $$X^3$$, 심지어 $$X^4$$를 추가하여 유연하게 할 수 있다. 그러나 편향이 감소하는 동시에 분산이 증가하기 때문에 성능이 반드시 더 나아질 것이라는 보장은 없다.