---
layout: post
title: ISL 7. Moving Beyond Linearity
subtitle: An Introduction to Statistical Learning
tags: [ISL]
use_math: true
---


지금까지 선형 모델에 대하여 공부했다. 그러나 실제로 선형성 가정이 지켜지는 경우는 거의 없기 때문에 일반적인 선형 회귀는 예측력 면에서 심각한 한계점을 가진다. 이번 장에서는 선형성 가정을 완화하면서 모델 해석 가능성은 유지하기 위한 여러 접근법들을 살펴볼 것이다.

먼저, **다항 회귀(polynomial regression)**는 원래의 예측변인들을 거듭제곱하여 선형 모델에 새로운 예측변인으로 추가하는 방법이다. **계단 함수(step function)**는 한 변인의 범위를 $$K$$개의 구역으로 잘라 질적 변인으로 만들고 구간적 상수 함수(piecewise constant function)에 적합시킨다. 한편, **회귀 스플라인(regression spline)**은 다항 회귀와 계단 함수를 합친 형태로, $$X$$의 범위를 $$K$$개의 구역으로 분리하고 각 구역 내에서 다항 회귀를 적합시킨다. 그러나 이러한 다항식들이 영역의 경계, 즉 노트(knot)에서 평활하게(smoothly) 이어지도록 한다. **평활 스플라인(smoothing spline)**은 잔차 제곱 합과 평활성 페널티를 동시에 최소화하는 결과로, 모든 데이터 포인트에서 노트를 가지는 자연 3차 스플라인의 일종이다. **로컬 회귀(local regression)**는 타깃 포인트에서의 적합을 계산하기 위하여 가까운 관측치들에 큰 가중치를 두는 방법이다. 마지막으로, **일반화 가법 모형(generalized additive models)**은 위 방법들을 여러 개의 예측변인에 대해서 확장시킨다.



## 7.1 다항 회귀

예측변인과 반응변인 간 비선형적 관계가 존재하는 상황으로 선형 회귀를 확장하기 위해서 **다항 회귀(polynomial regression)**는 선형 모델 $$y_i = β_0 + β_1x_i + \epsilon_i$$를 아래와 같은 다항 함수로 변환한다.


$$
y_i = β_0 + β_1x_i + β_2x^2_i + β_3x^3_i + . . . + β_dx^d_i + \epsilon_i
$$


이때, $$\epsilon_i$$는 오차항이다. 이는 $$x_i,\ x^2_i,\ x^3_i,\ ...,\ x^d_i$$를 예측변인으로 가지는 보통의 선형 모델과 같기 때문에 최소 자승 선형 회귀를 통해 계수들을 추정할 수 있다. $$d$$의 값이 3이나 4보다 커지면 다항 곡선이 지나치게 유연해지기 때문에 주의해야 한다.



## 7.2 계단 함수

**계단 함수(step functions)**는 $$X$$의 범위를 여러 빈(bin)으로 쪼개고 각 빈에서 서로 다른 상수를 적합시킨다. 즉, 연속형 변수(continuous variable)를 순서가 있는 범주형 변수(ordered categorical variable)로 변환하는 것이다. 구체적으로, $$X$$의 범위에서 컷포인트 $$c_1, c_2, . . . , c_K$$를 설정하고 아래의 $$K + 1$$개의 새로운 변인들을 만들 수 있다.

$$
C_0(X) = I(X <c_1), \\
C_1(X) = I(c_1 ≤X <c_2), \\
C_2(X) = I(c_2 ≤X <c_3), \\
.\\.\\.\\
C_{K−1}(X) = I(c_{K−1} ≤X <c_K),\\
C_K(X) = I(c_K ≤ X),\\
$$


$$I(·)$$는 조건이 참이면 1을 반환하고 거짓이면 0을 반환하는 지시 함수(indicator function)이다. 이러한 변인들은 **더미 변수(dummy variables)**라고도 불린다. 이때, $$X$$가 어떤 값을 가지든 이 $$K + 1$$개의 간격 중 하나에 속하기 때문에 $$C_0(X)+C_1(X)+. . .+C_K(X) = 1$$이다. 다음으로, $$C_1(X), C_2(X), . . ., C_K(X)$$를 예측변인으로 하는 아래의 선형 모델을 최소 자승법을 이용해 적합시킨다.


$$
y_i = β_0 + β_1C_1(x_i) + β_2C_2(x_i) + . . . + β_KC_K(x_i) + \epsilon_i
$$


$$X < c_1$$이면 모든 예측변인의 값이 0이기 때문에 $$β_0$$를 $$X < c_1$$에 대한 $$Y$$의 평균 값으로 해석할 수 있다. 한편, $$c_j ≤X <c_{j +1}$$이면 위 식은 $$β_0+β_j$$를 반응변인으로 예측하게 되고, 이때 $$β_j$$는 $$X < c_1$$에 비해 $$c_j ≤X <c_{j+1}$$에서의 $$X$$에 대한 반응변인의 평균 증가량을 의미한다.



## 7.3 기저 함수

위에서 살펴본 다항 회귀나 계단 함수를 이용한 구간적 상수(piecewise-constant) 회귀 모형은 **기저 함수(basis function)** 접근의 일종이다. 기저 함수란 $$b_1(X), b_2(X), . . . , b_K(X)$$와 같이 변인 $$X$$에 적용할 수 있는 함수나 변환(transformation)을 말한다. 즉, $$X$$에 대한 선형 모델 대신 아래와 같은 모델을 적합시키는 것이다.


$$
y_i = β_0 + β_1b_1(x_i) + β_2b_2(x_i) + β_3b_3(x_i) + . . . + β_Kb_K(x_i) + \epsilon_i
$$


이때, 기저 함수 $$b_1(·), b_2(·), . . . , b_K(·)$$는 미리 정해져 있어야 한다. 다항 회귀의 경우 기저 함수는 $$b_j(x_i) = x^j_i$$이며, 계단 함수의 경우에는 $$b_j(x_i) = I(c_j ≤ x_i < c_{j+1})$$이다. 이러한 식들은 예측변인이 $$b_1(x_i), b_2(x_i), b_3(x_i), . . ., b_K(x_i)$$인 보통의 선형 모델로 생각될 수 있기 때문에 최소 자승법을 통해 회귀 계수를 추정할 수 있다.



## 7.4 회귀 스플라인

**구간적 다항 회귀(piecewise polynomial regression)**는 $$X$$의 서로 다른 영역에서 별개의 저차(low-degree) 다항식을 적합시키는 것이다. 예를 들어, 구간적 3차 회귀 모형은 아래와 같은 형식을 취한다.


$$
y_i = β_0 + β_1x_i + β_2x^2_i + β_3x^3_i + \epsilon_i
$$


이때, 계수 $$β_0, β_1, β_2, β_3$$은 $$X$$의 범위의 서로 다른 구간에서 달라진다. 이렇게 계수가 변화하는 포인트들을 노트(knot)라고 부른다. 노트가 없는 구간적 3차 다항식은 일반적인 3차 다항식과 같다. 포인트 $$c$$에서 노트가 하나인 구간적 3차 다항식은 아래와 같다.


$$
y_i = \begin{cases}
β_{01} + β_{11}x_i + β_{21}x^2_i + β_{31}x^3_i + \epsilon_i & \mbox{if}\ x_i < c; \\
β_{02} + β_{12}x_i + β_{22}x^2_i + β_{32}x^3_i + \epsilon_i & \mbox{if}\ x_i ≥ c
\end{cases}
$$


즉, 데이터에 두 개의 서로 다른 다항함수를, 하나는 $$x_i < c$$인 관측치의 서브셋에, 다른 하나는 $$x_i ≥ c$$인 관측치의 서브셋에 적합시키는 것이다. 노트를 더 많이 사용할수록 구간적 다항식은 유연해진다. $$X$$의 범위에서 $$K$$개의 노트를 만들면 $$K+1$$개의 3차 다항식을 적합시키게 된다.

그러나 적합된 커브가 너무 유연하면 노트에서 커브가 끊기는 등의 문제가 발생할 수 있으므로 커브가 연속적일 수 있도록 구간적 다항식을 적합하는 데 제약을 걸 수 있다. 여기에 구간적 다항식의 1차 도함수(first derivative) 및 2차 도함수(second derivative)가 모두 연속이도록 하는 제약을 추가하면 커브가 더욱 평활해지며, 이러한 커브를 **3차 스플라인(cubic spline)**이라 부른다. 일반적으로, $$d$$차 스플라인은 각 노트에서 $$d-1$$차까지의 도함수가 모두 연속인 구간적 $$d$$차 다항식을 말한다. 한편, **선형 스플라인(linear spline)**은 노트로 분리되는 각 예측변인의 영역에서 직선을 적합시키되, 각 노트에서의 연속성을 지키는 것이다.

다항 회귀나 계단 함수처럼 회귀 스플라인 역시 기저 모델(basis model)로 표현할 수 있다. $$K$$개의 노트를 가지는 3차 스플라인은 아래와 같이 표현된다.


$$
y_i = β_0 + β_1b_1(x_i) + β_2b_2(x_i) + ... +β_{K+3}b_{K+3}(x_i) + \epsilon_i
$$


이때, $$b_1, b_2, . . . , b_{K+3}$$는 적절히 선택된 기저 함수이며, 모델은 최소 자승법을 통해 적합될 수 있다. 3차 스플라인을 표현하기 위하여 여러 가지 기저 함수를 사용할 수 있다. 가장 직접적인 방법은 3차 다항식의 기저 함수(즉, $$x, x^2, x^3$$)로 시작하여 아래와 같은 절단된 멱(truncated power) 기저 함수를 노트마다 하나씩 추가하는 것이다.


$$
h(x, ξ) = (x − ξ)^3_+ = \begin{cases}
(x − ξ)^3 & \mbox{if}\ x > ξ \\
0 & \mbox{otherwise}
\end{cases}
$$


이때, $$ξ$$는 노트를 나타낸다. 즉, $$y_i = β_0 + β_1x_i + β_2x^2_i + β_3x^3_i + \epsilon_i$$에 $$β_4h(x, ξ)$$와 같은 항을 추가해주면 함수와 1차 및 2차 도함수에서는 연속이지만, 3차 도함수에서는 $$ξ$$에서 불연속이 발생하게 된다. 정리하자면, $$K$$개의 노트를 두고 데이터셋에 3차 스플라인을 적합하려면, 절편과 $$X,X^2,X^3, h(X, ξ_1), h(X, ξ_2), . . . , h(X, ξ_K)$$라는 $$3+K$$개의 예측변인을 가지고 최소 자승 회귀를 수행해야 한다. 이것은 총 $$K+4$$개의 회귀 계수를 추정하는 것과 같기 때문에 $$K$$개의 노트를 가지는 3차 스플라인의 자유도는 $$K+4$$가 된다.

스플라인은 예측변인의 바깥쪽 범위에서는 분산이 커진다는 단점이 있다. 이를 보완하기 위하여 **자연 스플라인(natural spline)**은 $$X$$가 가장 작은 노트보다 작거나 가장 큰 노트보다 큰 영역에서 함수가 선형이도록 하는 제약을 가진다. 이로써 가장자리에서도 더 안정적인 추정치를 얻을 수 있다.

그렇다면 노트의 개수와 위치는 어떻게 선택하는 것이 좋을까? 보통 노트를 일정한 간격으로 두게 되는데, 자유도를 정하면 소프트웨어가 자동으로 상응하는 수의 노트를 데이터에서 균질한 분위수(quantile)에 배치해준다. 자유도는 교차검증을 사용해 RSS를 최소화하는 $$K$$의 값을 선택하여 결정한다.

회귀 스플라인은 앞 절에서 소개했던 다항 회귀보다 보통 더 좋은 결과를 낸다. 스플라인에서는 차수(degree)를 고정시킨 상태로 노트의 개수를 늘려 유연성을 증가시킬 수 있으며, 대개 더 안정적인 추정치를 제공한다. 또한, 함수 $$f$$가 빠르게 변화하는 구간에서는 노트를 더해 유연성을 높이고 $$f$$가 안정적인 구간에서는 적은 노트를 사용할 수도 있다.



## 7.5 평활 스플라인



### 7.5.1 평활 스플라인의 개관

우리는 관측된 데이터에 잘 적합되는 어떤 함수 $$g(x)$$를 찾고자 한다. 즉, $$RSS=\sum^n_{i=1} (y_i − g(x_i))^2$$을 줄여야 한다. 그러나 $$g(x_i)$$에 어떤 제약도 걸지 않는다면 모든 $$y_i$$에 보간(interpolate)하는 $$g$$를 선택하여 RSS를 0으로 만들 수 있을 것이다. 이러한 함수는 데이터에 완벽하게 과적합된다. 따라서 우리가 찾고 싶은 것은 RSS를 작게 하면서도 평활한(smooth) 함수 $$g$$이다. 이를 위해서 아래를 최소화하는 $$g$$를 찾을 수 있을 것이다.


$$
\sum^n_{i=1} (y_i − g(x_i))^2 + λ \int g''(t)^2dt
$$


여기에서 $$λ$$는 음수가 아닌 튜닝 파라미터이다. 위를 최소화하는 함수 $$g$$를 **평활 스플라인(smoothing spline)**이라고 한다. 위 식은 "손실(loss)+페널티" 형식으로, $$\sum^n_{i=1} (y_i − g(x_i))^2$$은 $$g$$가 데이터에 잘 적합되도록 하는 손실 함수이며, $$λ \int g''(t)^2dt$$는 $$g$$의 분산을  낮추는 페널티 항이다. $$g''(t)$$는 함수 $$g$$의 2차 도함수를 의미한다. 1차 도함수 $$g'(t)$$는 $$t$$에서 함수의 기울기를 측정하고, 2차 도함수는 그 기울기가 변화하는 양을 나타낸다. $$g(t)$$가 $$t$$ 근처에서 매우 꾸불꾸불하다면 2차 도함수의 절댓값이 커지며, 평활할수록 0에 가까워진다. 따라서 직선의 2차 도함수는 0이다. $$\int$$는 적분을 나타내며 $$t$$의 범위에서의 합이라고 생각할 수 있다. 정리하자면, $$\int g''(t)^2dt$$는 전체 범위에서 함수 $$g'(t)$$의 총 변화량을 측정한 것이라고 할 수 있다. $$λ$$는 평활 스플라인의 편향-분산 트레이드오프를 조정하는 역할을 수행한다. $$λ$$의 값이 클수록 $$g$$는 평활해진다.

평활 스플라인은 $$x_1, . . . , x_n$$의 고유한 값에 노트를 두는 구간적 3차 다항식의 일종으로, 각 노트에서 연속인 1차 및 2차 도함수를 가진다. 또한, 극단 노트의 바깥쪽에서 선형이다. 즉, 평활 스플라인은 $$x_i$$의 모든 고유한 값에서 노트를 가지는 자연 3차 스플라인이라고 할 수 있다. 단, 튜닝 파라미터 $$λ$$가 정규화 정도를 조절하는 특수한 버전이다.



### 7.5.2 평활 파라미터 $$λ$$ 선택하기

평활 스플라인이 모든 데이터 포인트에서 노트를 가지기 때문에 자유도가 지나치게 클 것으로 생각할 수 있다. 평활 스플라인은 $$n$$개의 파라미터를 가지고 $$n$$의 명목상(nominal) 자유도를 가지지만, 이 $$n$$개의 파라미터는 크게 제약되거나 축소된다. 따라서 실질적인(effective) 자유도 $$df_λ$$가 평활 스플라인의 유연성에 대한 지표로 사용된다. 실질적인 자유도는 튜닝 파라미터 $$λ$$를 통해 조절할 수 있으며, $$λ$$가 0에서 $$∞$$으로 증가함에 따라 $$n$$에서 2로 감소한다. 이제 수식을 통해 좀 더 전문적으로 살펴보자.

$$
\mathrm{\hat{g}_λ = S_λy}
$$


위 식에서 $$\hat{\mathrm{g}}$$은 특정 $$λ$$에 대한 평활 스플라인의 해(solution)로, 훈련 포인트 $$x_1, . . . , x_n$$에서 평활 스플라인의 적합된 값들을 포함하는 $$n$$차원 벡터이다. 위 식은 좌항의 적합된 값들의 벡터가 따로 공식을 가지는 $$n × n$$ 행렬 $$\mathrm{S_λ}$$ 곱하기 반응 벡터 $$\mathrm{y}$$로 표현될 수 있음을 보여준다. 여기에서 실질적인 자유도는 아래와 같이 행렬 $$\mathrm{S_λ}$$의 대각 성분(diagonal element)들의 합으로 정의된다.


$$
df_λ = \sum^n_{i=1}\left\{\mathrm{S_λ}\right\}_{ii}
$$


$$λ$$의 값은 교차검증 RSS를 가능한 한 작게 하는 것으로 한다. 참고로, 평활 스플라인에서는 leave-one-out 교차검증(LOOCV) 오류를 아래의 공식을 이용해 단 한 번 적합시키는 것과 같이 매우 편리하게 계산할 수 있다.


$$
RSS_{cv}(λ) = \sum^n_{i=1} (y_i − \hat{g}^{(−i)}_λ (x_i))^2 =
\sum^n_{i=1} \left[\frac {y_i − \hat{g}_λ(x_i)} {1 − \left\{S_λ\right\}_{ii}}\right]^2
$$


여기에서 $$\hat{g}^{(−i)}_λ(x_i)$$는 $$i$$번째 관측치 $$(x_i, y_i)$$만 제외한 다른 모든 훈련 관측치를 이용해 적합시킨 평활 스플라인으로 $$x_i$$에 대하여 예측한 값을 의미한다. 한편, $$\hat{g}_λ(x_i)$$는 모든 관측치를 이용해 적합시킨 평활 스플라인으로 $$x_i$$에 대하여 예측한 값이다. 정리하자면, 위 공식은 모든 데이터를 사용한 원래의 적합 $$\hat{g}_λ$$만 가지고도 leave-one-out 적합을 계산할 수 있다는 것이다.



## 7.6 로컬 회귀

**로컬 회귀(local regression)**는 가까운 관측치만을 이용해 타깃 포인트 $$x_0$$에서의 적합을 계산하는 또 다른 비선형 접근이다. 먼저, $$x_0$$로부터 가장 가까운 $$x_i$$를 가지는 훈련 포인트들의 일부 $$s = k/n$$를 모은다. 여기에 속하는 각 포인트에 $$K_{i0} = K(x_i, x_0)$$의 가중치를 부과하여 $$x_0$$로부터 가장 먼 포인트는 0의 가중치를, 가장 가까운 포인트는 가장 큰 가중치를 가지도록 한다. 이 $$k$$개의 최근접 이웃들(nearest neighbors)을 제외한 모든 포인트들은 모두 0의 가중치를 갖는다. 이 가중치들을 이용해 $$x_i$$의 $$y_i$$에 대한 가중 최소 자승 회귀를 적합시키고 아래를 최소화하는 $$\hat{β}_0$$과 $$\hat{β}_1$$을 찾는다.

$$
\sum^n_{i=1} K_{i0}(y_i − β_0 − β_1x_i)^2
$$


마지막으로, $$x_0$$에서 적합된 값인 $$\hat{f}(x_0) = \hat{β}_0 + \hat{β}_1x_0$$를 구한다. 이때, 가중치 $$K_{i0}$$는 $$x_0$$의 각 값에 따라 달라진다. 즉, 새로운 포인트에서 로컬 회귀를 수행하고자 한다면 새로운 가중치를 가지고 가중 최소 자승 회귀 모델을 적합시켜야 한다. 로컬 회귀는 최근접 이웃 방법과 마찬가지로 예측할 때마다 모든 훈련 데이터를 필요로 하기 때문에 기억(memory) 기반 절차라고 불리기도 한다.

로컬 회귀를 수행하기 위해서는 가중 함수 $$K$$를 어떻게 결정할지, 선형, 상수, 이차 회귀 중 어떤 것을 적합시킬지 등 다양한 결정을 내려야 한다. 그중 가장 중요한 것은 비선형 적합의 유연성을 조절하는 스팬(span) $$s$$를 결정하는 일이다. $$s$$의 값이 작을수록 적합은 국소적(local)이고 꾸불꾸불할 것이고, $$s$$의 값이 크면 더 많은 훈련 관측치들을 사용해 전역적(global)인 적합을 얻을 수 있을 것이다. 역시 교차검증을 통해 $$s$$를 선택할 수도 있고 아니면 직접 정해줄 수도 있다.

로컬 회귀는 여러 가지 상황에 적용될 수 있다. 한 예로, 여러 변인들 중 일부 변인들에는 전역적인 다중 선형 회귀 모델을 적합시키고 나머지 변인들에는 로컬 다중 선형 회귀를 수행하는 가변 계수(varying coefficient) 모델이 있다. 이는 최근 모아진 데이터에 모델을 적응시키는 유용한 방법이다.

로컬 회귀는 $$p$$가 3이나 4보다 훨씬 큰 상황에서 $$x_0$$에 가까운 훈련 관측치들이 적어지기 때문에 성능이 저하된다는 점에 유의하자.



## 7.7 일반화 가법 모형

지금까지 학습한 방법들은 단일 예측변인 $$X$$를 가지고 반응변인 $$Y$$를 예측하는 것이었다. 이 방법들은 단순 선형 회귀(simple linear regression)의 확장으로 이해할 수 있다. 이번 절에서는 복수의 예측변인 $$X_1, . . . , X_p$$를 가지고 $$Y$$를 예측하는 **일반화 가법 모형(generalized additive models; GAMs)**에 대해서 살펴본다. 즉, 다중 선형 회귀(multiple linear regression)의 확장에 해당한다. GAM은 가법성(additivity)을 유지하면서 각 변인들의 비선형 함수를 허용하는 전반적인 프레임워크이다.



### 7.7.1 회귀 문제에서의 GAMs

예측변인와 반응변인 간 비선형 관계를 허용하기 위하여 다중 선형 회귀 모델 $$y_i = β_0 + β_1x_{i1} + β_2x_{i2} + · · · + β_px_{ip} + \epsilon_i$$를 확장하는 자연스러운 방법은 각 선형 요소 $$β_jx_{ij}$$를 비선형 함수 $$f_j(x_{ij})$$로 대체하는 것이다. 아래와 같이 모델을 쓸 수 있다.


$$
y_i = β_0 + \sum^p_{j=1}f_j(x_{ij}) + \epsilon_i \qquad \qquad \qquad \qquad \qquad \quad \\
= β_0 + f_1(x_{i1}) + f_2(x_{i2}) + · · · + f_p(x_{ip}) + \epsilon_i
$$


이것은 GAM의 예이며, 각 $$X_j$$에 대하여 별개의 $$f_j$$를 계산하여 모두 더하기 때문에 가법(additive) 모형이라 부른다. 앞 절에서 학습했던 단일 변인을 이용하는 방법들을 토대로 가법 모형을 적합시킬 수 있다.

GAM을 사용하면 각 $$X_j$$에 대하여 비선형 함수 $$f_j$$를 적합시켜 자동적으로 비선형 관계를 모델링할 수 있기 때문에, 개별 변인에 일일이 여러 가지 변환(transformation) 방법들을 시도해볼 필요가 없다. 또한, 가법 모델이기 때문에 다른 변인들이 고정된(fixed) 상태로 $$Y$$에 대한 각 $$X_j$$의 영향을 측정할 수 있다. 변인 $$X_j$$에 대한 함수 $$f_j$$의 평활도(smoothness)는 자유도로 표현된다. GAM의 가법성 때문에 중요한 상호작용을 놓칠 위험성이 존재하지만, $$X_j × X_k$$와 같은 형태로 상호작용 항을 추가하거나, $$f_{jk}(X_j,X_k)$$ 형태로 로컬 회귀나 이차 스플라인 같은 저차원의 상호작용 함수를 GAM에 추가하여 보완할 수 있다. GAM은 8장에서 학습할 랜덤 포레스트나 부스팅과 같은 비모수적(nonparametric) 모델과 선형 모델 사이의 절충안을 제공한다.



### 7.7.2 분류 문제에서의 GAMs

$$Y$$가 질적(qualitative)인 경우에도 GAM을 사용할 수 있다. $$Y$$가 0 혹은 1의 값을 취한다고 가정하고, $$p(X) = Pr(Y =1 \vert X)$$를 예측변인이 주어졌을 때 반응변인이 1일 조건부 확률을 나타낸다고 해보자. 로지스틱 회귀 모형은 아래와 같다.


$$
\log \left(\frac {p(X)} {1 − p(X)} \right)
= β_0 + β_1X_1 + β_2X_2 + · · · + β_pX_p
$$


이러한 로짓(logit)은 $$P(Y = 1 \vert X)$$ 대 $$P(Y = 0 \vert X)$$의 오즈(odds)에 로그를 취한 값이다. 이것을 비선형 관계로 확장하는 방법은 아래와 같다.


$$
log \left(\frac {p(X)} {1 − p(X)} \right)
= β_0 + f_1(X_1) + f_2(X_2) + · · · + f_p(X_p)
$$


이것이 **로지스틱 회귀(logistic regression) GAM**이며, 이전 절에서 언급했던 GAM의 장점과 단점을 똑같이 따른다.

