---
layout: post
title: ISL 2. Statistical Learning
subtitle: An Introduction to Statistical Learning
tags: [ISL]
use_math: true
---





이번 장에서는 먼저 통계 학습의 정의에 대하여 배우고, 예측과 추론이라는 통계 학습의 두 가지 목적에 대하여 공부한다. 예측의 정확도와 추론의 용이성 사이에는 트레이드오프가 존재한다. 통계 학습에는 모수적 방법과 비모수적 방법이 있다.

두 번째 절에서는 회귀와 분류 상황에서 모델의 적합도를 평가하는 방법에 대하여 공부한다. 회귀에서는 MSE가, 분류에서는 오류율이 흔히 사용되는 척도이다. 또한 편향-분산 트레이드오프, 과적합과 같은 중요한 개념들을 학습할 것이다.



## 1 통계 학습이란 무엇인가?

통계 학습에서는 **예측변인(predictor)** (혹은 입력변수, 독립변인, feature) $$X​$$와 **반응변인(response)** (혹은 출력변수, 종속변인) $$Y​$$ 사이에 어떤 관계가 있음을 가정하며 이러한 관계는 아래의 식과 같이 표현될 수 있다.



$$
Y = f(X) + \epsilon
$$



위의 식에서 $$\epsilon$$ (epsilon)은 무선적인 오차항(random error term)이며, $$X$$와 독립이고 평균은 0이다. 결국 통계 학습이란 주어진 데이터를 가지고 $$f$$를 추정하는 것을 뜻한다.



### 1.1 왜?

통계 학습의 목적은 아래의 두 가지, 예측과 추론으로 정리될 수 있다.



#### 예측

오차항은 평균이 0이기 때문에 $$Y$$를 예측하기 위해서는 오차항을 빼고 단순히 $$f$$의 추정치에 $$X$$를 넣어주면 된다. 이는 아래와 같은 식으로 표현된다.


$$
\hat{Y} = \hat{f}(X)
$$

만일 예측만이 목적이라면 $$Y​$$를 정확하게 예측하기만 하면 $$\hat{f}​$$의 정확한 형태에 대해서는 별로 상관하지 않아도 된다. 예측의 정확도는 크게 두 가지 요소에 따라 좌우된다.

첫째, $$\hat{f}$$이 실제 $$f$$가 아닌 것에서 기인하는 **reducible error**이다. 추정의 과정에서 적절한 테크닉을 사용하면 줄일 수 있기 때문에 "reducible"이라는 이름이 붙었다.

둘째, $$X$$를 통해 $$Y$$를 완전히 예측할 수 없는 것에서 기인하는 **irreducible error**이다. $$\epsilon$$은 $$Y$$를 예측하는 데 도움이 될 수 있지만 측정되지 않은 변인의 영향을 포함하고 있다. 따라서 irreducible error는 $$\epsilon$$의 분산에 해당한다.



$$\hat{f}$$과 $$X$$가 고정되어 있다면 아래와 같은 식이 성립한다.


$$
E(Y-\hat{Y})^2 = E[f(X)+\epsilon-\hat{f}(X)]^2 \\
\qquad \qquad \quad \quad \ \ =[f(X)-\hat{f}(X)]^2+Var(\epsilon)
$$



위 식에서 좌변의 $$E(Y − \hat{Y} )^2$$은 실제 $$Y$$와 예측된 $$Y$$의 차이의 제곱의 기댓값이다. 우변의 $$[f(X)-\hat{f}(X)]^2$$은 reducible error에, $$Var(\epsilon)$$은 irreducible error에 해당한다. Irreducible error는 말 그대로 줄일 수 없는 오차이기 때문에 $$Y$$에 대한 예측에서 정확도의 상한선을 정해준다. 우리의 목표는 이 상한선 내에서 reducible error를 줄여 최대한의 예측의 정확도를 달성하는 것이다.



#### 추론

예측과 달리 추론이 목적이라면 $$\hat{f}$$의 정확한 형태를 파악해야 한다. 즉, $$X$$에 따라 $$Y$$가 어떻게 변화하는지 그 관계에 대하여 이해해야 한다.

어떤 예측변인이 반응변인과 어떻게 관련되어 있는지, 두 변인의 관계가 선형적인지 아닌지 등과 같은 질문이 이에 해당할 것이다.



### 1.2 어떻게?

$$f$$를 추정하기 위하여 사용하는 데이터를 훈련 데이터(training data)라고 한다. 이 데이터 내에서 $$x_{ij}$$라 함은 $$i$$번째 관측치($$i=1,2,...,n$$)의 $$j$$번째 예측변인($$j=1,2,...,p$$)에 해당하는 값을 의미한다. 한편, $$y_i$$라 함은 $$i$$번째 관측치에 대한 반응변인을 말한다. 그렇다면 훈련 데이터는 $$\left\{(x_1, y_1), (x_2, y_2), ... , (x_n, y_n) \right\}$$의 형태로 표현할 수 있을 것이다. (이때, $$x_i = (x_{i1}, x_{i2}, ... , x_{ip})^T$$)



우리의 목표는 각 관측치의 $$(X, Y)$$ 쌍에 대하여 $$Y ≈ \hat{f}(X)$$를 충족하는 $$\hat{f}$$을 찾는 것이다. 이를 위해서는 크게 모수적 방법(parametric methods)과 비모수적 방법(non-parametric methods)을 사용할 수 있다.



#### 모수적 방법

모수적 방법에서는 $$f$$의 함수적 형태를 미리 가정한 다음 훈련 데이터를 사용해 모델을 훈련하고 파라미터를 추정한다. 이 방법의 장점은 $$f$$를 추정하는 문제를 파라미터를 추정하는 문제로 축소할 수 있다는 것이다. 단점은 $$f$$의 진짜 형태를 모르는 상태에서 우리가 선택한 모델이 그에 잘 맞지 않을 수 있다는 것인데, 유연한(flexible) 모델을 선택함으로써 어느 정도 이러한 문제를 해소할 수 있다. 그러나 유연한 모델은 항상 과적합(overfitting) 문제를 안고 있다는 점에 유의하여야 한다.



#### 비모수적 방법

모수적 방법과 달리 비모수적 방법에서는 $$f$$의 함수적 형태를 미리 가정하지 않은 상태에서 데이터포인트들을 잘 반영할 수 있는 $$f$$의 추정치를 찾는다. 장점은 $$f$$에 보다 정확하게 모델을 적합시킬 수 있다는 것이지만, 정확한 추정을 위해서는 매우 많은 수의 관측치들이 필요하다고 한다.



### 1.3 예측 정확도와 해석 용이성 간의 트레이드오프

만일 통계 학습에서 추론을 목적으로 하고 있다면, 심플한 모델을 사용하는 것이 해석의 용이성을 높인다. 지나치게 유연한 모델을 사용하면 $$f$$가 매우 복잡한 형태로 추정되어 개별 예측변인들이 반응변인과 어떤 연관성을 가지는지 이해하기 어려워지기 때문이다.

그러나 예측을 목적으로 할 때도 유연한 모델이 항상 좋은 성능을 보이는 것은 아니다. 과적합 때문에 오히려 예측의 정확도가 떨어질 수도 있다.



### 1.4 회귀 대 분류 문제

반응변인은 연속적인 숫자로 표현되어 정량적(quantitative)일 수도 있고 이산적으로 표현되어 정성적(qualitative; categorical)일 수도 있다. 전자의 경우는 **회귀(regression)** 문제로 접근하고 후자의 경우는 **분류(classification)** 문제로 접근할 수 있다.



## 2 모델의 정확도 평가



### 2.1 적합도 측정하기

예측값이 실제 데이터에 얼마나 잘 부합하는지 평가하기 위하여 회귀 상황에서 가장 흔히 사용하는 척도는 **평균 제곱 오차(mean squared error; MSE)**이다.



$$
MSE = \frac{1}{n} \sum_{i=1}^n(y_i-\hat{f}(x_i))^2
$$



트레이닝 데이터에서 계산된 MSE를 trianing MSE라고 한다. 그러나 우리가 정말 알고 싶은 것은 훈련 셋이 아니라 우리가 한 번도 본 적 없는 새로운 테스트 셋에서의 MSE일 것이다. 안타깝게도 trianing MSE가 낮다고 해서 test MSE도 항상 낮은 것은 아니다.

모델의 유연성이 높아질수록 trianing MSE는 지속적으로 감소하지만, test MSE는 대개 어느 정도까지는 감소하다가 다시 높아지는 U자형 곡선을 그린다. **과적합(Overfitting)**이란 training MSE는 낮지만 test MSE는 높은 상황을 일컫는다.

실제로 우리는 테스트 데이터에 접근할 수 없기 때문에 교차 검증(cross-validation)과 같은 방법을 사용하여 가지고 있는 훈련 데이터로부터 test MSE를 추정한다.



### 2.2 편향-분산 트레이드오프

$$x_0​$$에 대한 test MSE의 기댓값은 세 가지 요소의 합으로 표현 될 수 있다.


$$
E(y_0-\hat{f}(x_0))^2 = Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2 + Var(\epsilon)
$$



위의 식에서 **분산(variance)**은 우변의 첫 번째 항에 해당하며, 다른 훈련 데이터셋을 사용하여 추정할 때 $$\hat{f}$$이 변화하는 양을 뜻한다. 훈련 셋이 조금씩 바뀌더라도 $$f$$에 대한 추정치가 지나치게 변동하지 않는 것이 바람직할 것이다.

한편, **편향(bias)**은 우변의 두 번째 항에 해당하며, 복잡한 실생활의 문제를 훨씬 간단한 모델로 근사함에 따라 발생하는 오차를 뜻한다. 

우리의 목표는 좌변의 test MSE의 기댓값을 최소화하는 것이기 때문에우변의 세 번째 항에 해당하는 irreducible error를 제외하고 분산과 편향을 동시에 감소시킬 수 있다면 좋을 것이다. 그러나 보통 모델이 유연할수록 편향은 감소하지만 분산은 증가하는 트레이드오프가 발생한다.



### 2.3 분류 문제에서

지금까지는 회귀 문제를 기준으로 설명했지만, 대체로 많은 내용들이 분류 문제에도 똑같이 적용된다. 가장 큰 차이점은 반응변인이 연속형이었던 회귀에서 $$\hat{f}$$의 정확도를 평가하기 위하여 MSE를 사용하는 반면, 분류에서는 반응변인이 범주형이기 때문에 전체 중 반응변인을 잘못 예측한 오류의 수를 비율로 표현한 **오류율(error rate)**을 흔히 사용한다는 것이다.



$$
\frac{1}{n} \sum_{i=1}^n I(y_i \ne \hat{y}_i)
$$



위의 식에서 $$\hat{y}_i$$은 $$\hat{f}$$을 사용하여 $$i$$번째 관측치의 라벨을 예측한 것이며, $$I(y_i \ne \hat{y}_i)$$은 $$y_i \ne \hat{y}_i$$일 때는 1이 되고 $$y_i = \hat{y}_i$$일 때는 0이 되는 indicator variable이다.



#### 베이즈 분류기

오류율을 최소화하는 분류 방법은 각 관측치의 예측된 값에 근거하여 가장 가능도가 높은 클래스로 해당 관측치를 분류하는 것이다. 즉, 예측변인 벡터 $$x_0$$를 갖는 관측치를 조건부 확률 $$Pr(Y=j|X=x_0)$$이 최대가 되는 클래스 $$j$$로 분류하는 것이다.

이때, 조건부 확률이 정확히 50%가 되는 지점을 베이즈 결정 경계(Bayes decision boundary)라 하고, 베이즈 분류기에서의 테스트 오류율을 베이즈 오류율(Bayes error rate)이라 한다. 수식으로는 아래와 같이 표현할 수 있다.



$$
1−E (max_j Pr(Y = j|X))
$$



이러한 베이즈 오류율은 도달 가능한 최소한의 오류율이며 irreducible error에 해당한다.



#### K-최근접 이웃

실제로 우리는 위와 같은 조건부 확률을 직접 알 수 없다. 따라서 베이즈 분류기는 실제로 사용될 수 없고 가장 이상적인 분류기로 상정된다. 실제로 활용되는 분류 방법들은 조건부 확률을 추정하고 이에 기반하여 가장 추정된 확률이 높은 클래스로 관측치들을 분류한다.

K-최근접 이웃(K-nearest neighbors; KNN) 방법은 훈련 데이터에서 $$x_0$$에 근접한 K개의 포인트들을 찾아 $$\mathcal{N}_0$$로 표시하고 $$\mathcal{N}_0$$ 중 반응변인이 $$j$$인 포인트의 비율로 클래스 $j$에 대한 조건부 확률을 추정한다. 식은 아래와 같다.



$$
Pr(Y = j|X = x_0) = \frac{1}{K} \sum_{i∈\mathcal{N}_0} I(y_i = j)
$$



KNN은 베이즈 룰을 적용하여 추정된 조건부 확률이 가장 높은 클래스로 테스트 관측치 $$x_0​$$를 분류한다.

KNN 분류기의 성능은 K를 어떻게 정하는지에 따라 좌우된다. K가 1이 경우에는 결정 경계가 지나치게 유연해진다. 즉, 편향은 낮지만 분산이 크다. K가 커짐에 따라 유연성이 감소하고 결정 경계는 점차 선형적으로 변화한다. 즉, 편향이 높아지고 분산이 작아진다.

회귀에서와 마찬가지로 유연성이 증가할수록 훈련 오류율은 지속적으로 감소하는 반면, 테스트 오류율은 U자형 곡선을 그린다. 따라서 적정한 수준의 유연성을 정하는 것은 회귀나 분류 모두에서 중요하다.



